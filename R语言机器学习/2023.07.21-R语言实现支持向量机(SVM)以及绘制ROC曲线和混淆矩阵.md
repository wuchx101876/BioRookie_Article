**支持向量机（Support Vector Machine，SVM）**是一种经典的监督学习算法，用于进行分类和回归任务。SVM的原理基于统计学习理论中的结构风险最小化原则，旨在找到一个最优的超平面（或者曲面），将不同类别的样本尽可能地分开。

![支持向量机(SVM)](https://files.mdnice.com/user/23696/2762c7b5-c247-4311-88fb-fa9d9f4b3166.png)

## 支持向量机(SVM)优势
- **高效处理高维特征空间**：SVM在高维空间中计算样本之间的内积，并且通过核函数可以将样本映射到更高维的空间中。这使得SVM在处理高维特征数据时表现出色，例如在自然语言处理中的文本分类任务和图像识别中。
- **可以处理非线性问题**：通过使用核函数，SVM可以将数据映射到非线性空间中，从而在原始特征空间中线性不可分的问题上建立有效的决策边界。这使得SVM能够处理各种复杂的问题，并具有较好的分类性能。
- **可解释性强**：SVM的决策边界是由支持向量确定的，这些支持向量是最靠近决策边界的样本点。因此，SVM能够提供对分类决策的解释，并且可以通过支持向量的权重来理解特征的重要性。
##### 其特点可以与之前介绍的两个机器学习模型进行比较，选择自己合适的模型使用。
> [R语言实现逻辑回归(LR)以及绘制ROC曲线和混淆矩阵](https://mp.weixin.qq.com/s?__biz=Mzg2NjYzNjQ4Ng==&mid=2247485845&idx=1&sn=573ba5c4c0f1cff15c7d5bdee6926778&chksm=ce468e3cf931072ad013aafb100f3da30975b4ec7f6e2fbb87560de7b305d98b4dbbd8f0845c&token=350331289&lang=zh_CN#rd)

> [R语言实现随机森林(RF)以及绘制ROC曲线和混淆矩阵](https://mp.weixin.qq.com/s?__biz=Mzg2NjYzNjQ4Ng==&mid=2247485894&idx=1&sn=b58c4e2701d4ddd69cbde2ade32f6d46&chksm=ce468e6ff93107796e9f5870a4606457733c0d5b40f7bf5da7380d82c134066248d2c8e3b4f6&token=350331289&lang=zh_CN#rd)

#### 下面讲一下如何在R中实现**支持向量机**
## R语言实现SVM
#### 测试数据和代码
链接：https://pan.baidu.com/s/1P3S0FzQwdJ-ooafbyHQeFQ 
提取码：o4fy
```r
# 导入必要的包,没有安装的可以先安装一下
library(dplyr) #数据处理使用
library(data.table) #数据读取使用
library(e1071) #SVM模型使用
library(caret) # 调参和计算模型评价参数使用
library(pROC) #绘图使用
library(ggplot2) #绘图使用
library(ggpubr) #绘图使用
library(ggprism) #绘图使用
# 读取数据
data <- fread("./SVM_data.txt",data.table = F)  # 替换为你的数据文件名或路径
```

数据长这个样子，一共35727行，214列。每一行代表一个样本，第一列是样本标签`malignant`或`normal`，后面213列是213个特征。我们想根据213个特征，使用RF训练出一个能够对样本进行精准分类的模型。

![数据情况](https://files.mdnice.com/user/23696/fd2c11b4-33bf-4806-84e9-8699cd26fa24.png)

#### 构建SVM模型
```r
# 分割数据为训练集和测试集
set.seed(123)  # 设置随机种子，保证结果可复现
split <- sample.split(data$type, SplitRatio = 0.8)  # 将数据按照指定比例分割
train_data <- subset(data, split == TRUE)  # 训练集
test_data <- subset(data, split == FALSE)  # 测试集

# 定义训练集特征和目标变量
X_train <- train_data[, -1]
y_train <- as.factor(train_data[, 1])

# 创建并训练SVM模型
svm_model <- svm(x = X_train, y = y_train)

# 在训练集上进行预测
train_predictions <- predict(svm_model, newdata = X_train)

# 输出预测结果
table(y_train, train_predictions)
```

![初始模型效果](https://files.mdnice.com/user/23696/7250d7e6-025d-4ce9-9528-f5cd59ec2074.png)

通过混淆矩阵可以看出，模型预测效果还不错。接下来我们进一步进行调参。

#### 使用**caret包**进行调参
调整caret包提供据参数`sigma`和`C`。
- sigma（svm中的gamma）是径向基函数（Radial Basis Function，RBF）核函数的一个参数。它控制了数据在特征空间中的分布程度。较小的sigma值会导致模型更容易欠拟合，而较大的sigma值会导致模型更容易过拟合。

- C(svm中的cost)是SVM中的惩罚参数，它控制了误分类样本的惩罚程度。较小的C值表示允许更多的误分类，使得决策边界更容易适应训练数据，但可能会导致模型的泛化能力较差；较大的C值表示更严格地对误分类进行惩罚，使得决策边界更严格，但可能导致模型对训练数据过度拟合。

通过调节sigma和C这两个参数，可以控制SVM模型的复杂度和泛化能力。在参数调节过程中，通常需要通过交叉验证等技术来选择最佳的sigma和C取值，以优化模型性能。
```r
# 参数调整
# 创建参数网格
param_grid <- expand.grid(
  sigma = c(0.1, 1, 10),
  C = c(0.1, 1, 10))
# 定义交叉验证的控制参数,这里使用5折交叉验证
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)

# 进行参数调节
tuned_model <- train(
  x = X_train,
  y = y_train,
  method = "svmRadial",
  tuneGrid = param_grid,
  trControl = ctrl)

# 输出最佳参数配置
print(tuned_model)
```
给定的参数中最佳参数是`sigma = 0.1，c = 10。`其实效果还不如默认参数即`sigma = 1/特征数量`，`c = 1` 。**主要原因是参数范围选择不当造成。这里只是举一个如何调参的例子，所以结果不重要。**

![参数配置](https://files.mdnice.com/user/23696/7b6ae099-afe5-457d-8955-6feafa446202.png)

```r
# 使用最佳参数训练模型
svm_final_model <- svm(x = X_train, y = y_train)

# 在测试集上使用最佳参数配置进行预测
X_test <- test_data[, -1]
y_test <- as.factor(test_data[, 1])
test_predictions <- predict(svm_final_model, newdata = test_data)


# 输出预测结果
table(y_test, test_predictions)

# 计算评价模型指标。
# 准确率（Accuracy）
accuracy <- mean(test_predictions == y_test)
# 精确率（Precision）
precision <- sum(test_predictions == "normal" & y_test == "normal") / sum(test_predictions == "normal")
# 召回率（Recall）
recall <- sum(test_predictions == "normal" & y_test == "normal") / sum(y_test == "normal")
# F1值（F1-score）
f1_score <- 2 * precision * recall / (precision + recall)
# 输出指标结果
print(paste("准确率:", accuracy))
print(paste("精确率:", precision))
print(paste("召回率:", recall))
print(paste("F1值:", f1_score))
```
模型在测试集中的表现也不太好，**原因和上面一致，较大的sigma值导致模型过拟合。**

![模型在测试集中的效果](https://files.mdnice.com/user/23696/b7fd2533-778f-48d1-a89c-b79ba7e5221b.png)


#### 其它参数的调节
除了`gamma`和`cost`参数，SVM还有其它参数可以进行手动调节，具体可以参考随机森林(RF)参数调节的内容。这里简单介绍一下还有哪些参数。
- kernel：核函数类型。SVM可以使用不同的核函数来建立非线性决策边界。常见的核函数包括线性核函数（"linear"）、多项式核函数（"polynomial"）、径向基函数核函数（"radial"）、神经网络核函数（"Sigmoid"）等。通过选择适当的核函数，可以更好地拟合非线性关系。
- degree：多项式核函数的次数。当使用多项式核函数时，可以通过调节degree参数来控制多项式的次数。较高的次数可以使决策边界更复杂，但也可能导致过拟合。默认值是3
- coef0：核函数中的常数项。对于多项式核函数和Sigmoid核函数，coef0参数用于控制非线性部分的影响力。默认值是0
- type：SVM-分类机：C-classification。共五种：C-classification，nu-classification，one-classification(for novelty detection) ，eps-regression， nu-regression。
  - C-classification：这是一种常见的SVM分类模型，用于解决二分类或多分类问题。它通过寻找一个最优的超平面来将不同类别的样本分开。
  - nu-classification：这是另一种SVM分类模型，用于解决二分类或多分类问题。nu-classification引入了一个新的参数nu，它控制了支持向量的比例。这个参数允许在模型中使用的支持向量的数量有一定的灵活性。
  - one-classification（用于新颖性检测）：这是一种SVM模型，用于检测新颖或异常样本。在这种模型中，SVM试图构建一个边界，将训练样本集合内的样本与其余数据区分开来。新颖性检测模型主要用于检测与训练数据集不同的未知样本。
  - eps-regression：这是一种SVM回归模型，用于解决回归问题。它通过找到一个最优的超平面，使得大部分训练样本落在超平面的一个ε范围内。
  - nu-regression：这是另一种SVM回归模型，用于解决回归问题。nu-regression引入了一个新的参数nu，它控制了支持向量的比例。这个参数允许在模型中使用的支持向量的数量有一定的灵活性。

## 绘制ROC曲线和混淆矩阵
这部分内容可以参考逻辑回归(LR)和随机森林(RF)推文中的代码，都是一样的。**需要注意的是，绘制ROC曲线需要输出概率值**。这需要在训练模型时设置`probability = TRUE`。
```r
# 创建SVM模型，并启用概率估计
svm_model <- svm(x = X_train, y = y_train, probability = TRUE)

# 在测试集上进行预测
test_predictions <- predict(svm_model, newdata = X_test)

# 获取样本属于各个类别的概率估计值
prob_estimates <- attr(test_predictions, "probabilities")

# 输出概率估计值
print(prob_estimates)
```
关于R语言实现支持向量机(SVM)就分享这些，如果有时间，可能会更新模型的数学原理。


