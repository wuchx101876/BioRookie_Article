>> 最近比较忙，可能更新的频率会低一些，忙完这一段就恢复正常啦。今天更新一下t-SNE降维的R语言实现，以及原理。**做单细胞降维聚类的时候，经常会使用UMAP或t-SNE进行降维**，刚好学习一下他们的原理，便于我们更好的理解和处理数据。

##### t-SNE降维的R语言实现比较简单（处理单细胞的数据包一般会自带），先放前面，原理放到后面给大家看。

## R语言实现t-SNE降维

可以使用Rtsne包来进行t-SNE降维，这里用它对iris数据集进行降维

```r
# BiocManager::install("Rtsne")
library(Rtsne)
iris_unique <- unique(iris) #去重复
tsne_out <- Rtsne(as.matrix(iris_unique[,1:4])) #运行t-SNE
plot(tsne_out$Y,col=iris$Species,asp=1)
```
##### 降维效果还不错，看一下一些参数

| 参数    |   功能  |
| --- | --- |
| dims    | 	参数设置降维之后的维度，默认是2    |
|  perplexity    |   困惑度，参数须取值小于(nrow(data)-1)/3  |
|   theta  |  	参数越大，结果的准确度越低，默认是0.5   |
|  max_iter   |  最大迭代次数   |
|  pca   |   表示是否对输入的原始数据进行PCA分析，然后用分析后的数据进行后续分析
  |

![](https://files.mdnice.com/user/23696/9b202d7a-fbf0-41c2-b6c6-88f56547cba9.png)

##### 用MNIST数据集进行t-SNE测试，需要这个数据的可以后台回复"**t-SNE**"获得
```r
train<- read.csv("train.csv") 
library(Rtsne)
Labels<-train$label
train$label<-as.factor(train$label)
colors = rainbow(length(unique(train$label)))
names(colors) = unique(train$label)
tsne <- Rtsne(train[,-1], dims = 2, perplexity=30, max_iter = 500)

## 绘图
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=train$label, col=colors[train
```

![](https://files.mdnice.com/user/23696/459f92f1-9f15-4b2b-8c1c-1f1dfc7037da.png)

##### 也可以选择3维

```r
tsne <- Rtsne(train[,-1], dims = 3, perplexity=30, max_iter = 500)
# install.packages("rgl")
library(rgl)
plot3d(tsne$Y,labels=train$label, col=colors[t
```

![](https://files.mdnice.com/user/23696/23445ceb-f707-43ac-bc49-3981a3e9e4e1.png)

##### 我们后面来看一下降维原理

## 降维的分类

### 降维

降维：就是指采用某种映射方法，**将原高维空间中的数据点映射到低维度的空间中**。降维的本质是学习一个映射函数f: x→y，其中x是原始数据点的表达。 y是数据点映射后的低维向量表达。f可能是显式的或隐式的、线性的或非线性的。

### 降维方式

降维方式主要分为两个部分，**线性降维（PCA等）和非线性降维（SNE等）**

![](https://files.mdnice.com/user/23696/a2f7ec1d-8e31-4de7-80e5-8a026d1adc87.png)


### 线性降维
- PCA：目标是通过某种线性投影，将高维的数据映射到低维的空间中，并**期望在所投影的维度上数据的方差最大**，以此使用较少的维度，同时保留较多原数据的维度。
- LDA：线性判别分析LDA是一种有监督的线性降维算法,它的数据集的每个样本是有类别输出的,而PCA是不考虑样本类别输出的无监督降维技术。**LDA与PCA不同**，它是为了使降维后的数据点尽可能地容易被区分，它的思想用一句话概括，**就是“投影后类内方差最小，类间方差最大”。**

![](https://files.mdnice.com/user/23696/3971d9ad-e8fe-436b-9a41-942b1d80a323.png)

![](https://files.mdnice.com/user/23696/874f11c2-3df8-4a06-af11-5e5ecd135372.png)

### 非线性降维

**有时候数据之间的非线性关系是很重要的**，这时候我们用线性降维会得到很差的结果。

#### 全局
  - ISOMAP：(Isometric Feature Mapping, 等距离特征映射),其基于度量MDS,试图保留数据内在的由测地线距离蕴含的几何结构。其算法步骤为，**先构建邻接图，然后计算最短路径，最后通过MSD构建低纬的数据嵌入**。

#### 局部
  - LLE：局部线性嵌入(Locally Linear Embedding，LLE)是非常重要的降维方法。和传统的PCA，LDA等关注样本方差的降维方法相比，**LLE关注于降维时保持样本局部的线性特征**。
  
![两者区别](https://files.mdnice.com/user/23696/f7ff89e8-2a38-4038-976f-8fa17b271478.png)

### 不同降维的优缺点

![](https://files.mdnice.com/user/23696/ce54258d-8e64-40c6-9271-f65cb2050ca1.png)


## t-SNE降维原理

**t-SNE（t-Distributed 随机邻域嵌入）**，将数据点之间的相似度转换为概率。原始空间中的相似度由高斯联合概率表示，嵌入空间的相似度由“学生t分布”表示。**如果要准确的可视化样本间的相似度关系**，t-SNE表现更好。因为t-SNE主要是关注数据的局部结构。

### 数学原理
##### 同时保留局部结构，除了计算距离外，还将其转换为概率

![数学公式](https://files.mdnice.com/user/23696/abf667ce-9d94-4f5b-8586-0432c7c70d38.png)

### SNE（t-SNE前身）

#### 相似性计算
先计算原始空间（高维）的数据的相似性，**通过计算每个点和其它点之间的距离**，i是资料点，j是除了i以外的其它资料点。计算完之后，将其放入高斯方程，通过高斯分布计算点j为点i邻居的可能性。**在低维空间随机计算yi和yj。获得低维空间上i和j是邻居的概率，让在低维空间上j个i两个点的概率和高维空间上两个点的概率越相似越好**。


![](https://files.mdnice.com/user/23696/1b322948-34a3-4bf5-97fe-09675fc2177c.png)

#### 损失函数
**使用KL距离来衡量高维空间的分布和低维空间分布的相似程度**


![](https://files.mdnice.com/user/23696/29f7a43c-6f69-4979-b159-2f1422e8cfeb.png)

#### KL距离

KL距离，是Kullback-Leibler差异（Kullback-Leibler Divergence）的简称，也叫做相对熵（RelativeEntropy）。**它衡量的是相同事件空间里的两个概率分布的差异情况**。公式为：

![](https://files.mdnice.com/user/23696/1c5d2e78-3385-48ac-ba6b-b18b73695bbb.png)

### SNE的缺点

SNE降维之后，**可能会存在拥挤的问题**，导致即使可以从高维降低到低维，但是仍然无法分辨。

![](https://files.mdnice.com/user/23696/6d59010d-a871-4f3d-a0eb-7fb6e5b857e9.png)

### 用t分布解决

##### 高斯分布的情况下，远点到3/4的概率在数字上可能差的很小，导致无法有效的区分开3/4的差别。转换成t分布之后，可以有效的区分开远距离的点。

![](https://files.mdnice.com/user/23696/30b68b7a-58f4-44b3-976a-6770f3c77aa1.png)

### t-SNE
##### 高维空间仍然用高斯分布，在低维空间用t分布

![](https://files.mdnice.com/user/23696/909f584a-bd83-4778-a5fe-4c2803441a39.png)

- 决定分布情况的参数：σ
- σ太大：越拥挤，无法将点有效区分开
- σ太小：越离散，不能保留高维数据的局部结构

### 困惑度

困惑度可以解释为一个点附近的有效近邻点个数。**SNE对困惑度的调整通常选择5-50之间**，给定之后，使用二分搜索的方式寻找合适的σ


![](https://files.mdnice.com/user/23696/5ed2270c-29e3-4a4a-9a32-695dc801fba5.png)

### 损失函数

![](https://files.mdnice.com/user/23696/f0ca7c08-5119-4b55-9ba9-1bca450e89ab.png)

## t-SNE过程

#### 举个例子来直观的理解t-SNE过程

以二维空间为例，**假设我们现在需要将二维平面上分布的点降到一维直线上**，如果我们直接将这些点投放在x轴或者y轴，不同颜色/cluster的点会会混合在一起。

![](https://files.mdnice.com/user/23696/1a0d66e8-666e-4dfe-aa40-d282a555f75a.png)

### 第一步
##### 计算二维平面上所有点的相似性(similarity)。

- 首先计算黑色点和其周围点的距离(此处暂时以两个点为例，黑色和蓝色)，然后将这两个点排放在以黑色点为中心的正态曲线下，**接下来计算蓝色点到正态曲线的长度(unscaled similarity distance，也称为similarity score)。**
- 进而，我们获得了黑色点同其余所有点的similarity scores。**similarity score大，表明两个点在二维平面上的距离近**；相对的，表明两个点在二维平面上的距离远。


![](https://files.mdnice.com/user/23696/d0af0259-c5a3-4cd8-90a1-6252643f4104.png)

#### 第二步
##### 获得黑色点同其余所有点的similarity score后，我们需要对这些scores进行标准化，使得它们加和为1。

- 为什么要进行标准化处理呢？**假设平面上有一个新紫色的cluster(我们暂且认为紫色cluster的分布和蓝色cluster完全一样，只是密度(density)是蓝色cluster的两倍**，那么紫色cluster正态曲线的宽度也会是蓝色cluster的两倍(正态曲线的高度和宽度由方差决定))，
- 进行标准化处理后，这两个cluster的similarity score就是一样的了(**t-SNE可以同时保留数据的局部和全局结构**)。
- 接下来我们会获得每一个点同其余所有点的scaled similarity scores，**scaled similarity scores代表cluster的相对紧密度**。



![](https://files.mdnice.com/user/23696/0eb397d0-bac9-4190-88b4-3a90ec389e88.png)

### 第三步
- 由于每一个点所对应正态曲线的宽度是由其周围点分布的紧密度来决定的。那么两个点之间，前后两次计算的similarity scores可能会不同。所以t-SNE会将这两个点的similarity score求均值。最终，**我们可以获得一个矩阵，每一行/列表示这个点同其他点的similarity score**。


![](https://files.mdnice.com/user/23696/af2d434d-595b-4557-9e78-6b0c7c98a820.png)

### 第四步
##### 计算一维直线上所有点的similarity scores。
- 同之前计算二维平面上点的计算过程一般，选择一个指定的点，然后计算其同周围点的距离，进而获得similarity scores。**只是这次使用的曲线从正态分布变为t分布**。此时获得的矩阵比上面那个矩阵显得混乱一些。t-SNE每次移动一下直线上的点，移动的目的是为了让上图左边的矩阵变得像右边一样。

![](https://files.mdnice.com/user/23696/d17811ed-408a-49e1-a4c9-53a4ead40879.png)

## t-SNE优缺点
### 优点
- 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。
- 这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。

### 缺点
- 主要用于可视化，很难用于其他目的。
- **t-SNE倾向于保存局部特征**，对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间
- 全局结构未明确保留。**这个问题可以通过PCA初始化点（使用init ='pca'）来缓解**。
- **计算量大**，耗时间是PCA的百倍，内存占用大。

## t-SNE应用
- `单细胞RNA测序数据分析`：t-SNE 可以将单细胞RNA测序数据降维到二维或三维空间。
- `蛋白质结构可视化`：t-SNE可以用于降维蛋白质结构数据，**将高维的结构特征表示成二维或三维空间中的点。**
- `代谢组学分析`：代谢组学研究涉及大量的代谢产物，而这些产物往往是高维的。t-SNE 可以用于降维代谢组学数据。
- `疾病分类和亚型识别`：t-SNE 可以用于降维临床数据，如病人的基因表达数据、代谢数据或生物标志物数据，从而帮助发现疾病的亚型。

#### 以上就是关于降维和t-SNE降维的原理，有机会再学习一下UMAP的降维原理，对比一下两者再单细胞中的应用。